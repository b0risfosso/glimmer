{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4794a518-6206-4b28-8c7e-bea770c58d46",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cannot initialize model with low cpu memory usage because `accelerate` was not found in the environment. Defaulting to `low_cpu_mem_usage=False`. It is strongly recommended to install `accelerate` for faster and less memory-intense model loading. You can do so with: \n",
      "```\n",
      "pip install accelerate\n",
      "```\n",
      ".\n",
      "Loading pipeline components...: 100%|████████████████████████████████████████████████████| 7/7 [00:04<00:00,  1.49it/s]\n"
     ]
    }
   ],
   "source": [
    "from diffusers import StableDiffusionPipeline\n",
    "import torch\n",
    "\n",
    "# Load the Stable Diffusion model from Hugging Face\n",
    "model_id = \"CompVis/stable-diffusion-v1-4\"\n",
    "pipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float32)\n",
    "pipe = pipe.to(\"cpu\")  # Use \"cpu\" for inference since you don't have a GPU\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "96cfe79e-2428-4e18-a51e-547621fa6d17",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['parts that feel welcoming and sharp edges that show seriousness .']\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 50/50 [03:01<00:00,  3.63s/it]\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"A large, dark sculpture that sits on the grass. \n",
    "Shaped like a mushroom, or a cloud frozen in metal, made of bronze, \n",
    "has a deep brown color that seems to change with sunlight, sometimes lighter, sometimes black. \n",
    "the surface is smooth and a little rough with parts reflecting light and parts in the shadow. \n",
    "It is divided into sections like it has layers, \n",
    "with rounded parts that feel welcoming and sharp edges that show seriousness.\"\"\"\n",
    "image = pipe(prompt).images[0]  # Generates an image based on the prompt\n",
    "\n",
    "# Display the image\n",
    "image.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed1bf4cc-b0c5-49eb-aaef-62b2ae7a9754",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ef7aca65-f88f-4f00-a0d9-1cecaacedfe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "image.save(\"firstfirst.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f3d96e5-8570-43e1-ad32-c18dfb276e91",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from PIL import Image\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, image_paths, prompts, transform=None):\n",
    "        self.image_paths = image_paths\n",
    "        self.prompts = prompts\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = Image.open(self.image_paths[idx]).convert(\"RGB\")\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        prompt = self.prompts[idx]\n",
    "        return image, prompt\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5]),\n",
    "])\n",
    "\n",
    "# Example usage:\n",
    "# image_paths = [\"path/to/image1.jpg\", \"path/to/image2.jpg\", ...]\n",
    "# prompts = [\"A description for image 1\", \"A description for image 2\", ...]\n",
    "dataset = CustomDataset(image_paths, prompts, transform=transform)\n",
    "dataloader = DataLoader(dataset, batch_size=1, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a080b928-cd71-4b46-a42d-97e7b6f8fb6b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0rc2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
